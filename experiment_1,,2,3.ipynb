{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVeX088cFqa9",
        "outputId": "3536e68a-38f7-4ef2-d9b6-2913ca74f6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Natural Language Processing is very interesting\n",
            "Word Tokens: ['NaturalLanguageProcessingisveryinteresting', '']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"Natural Language Processing is very interesting\"\n",
        "words = []\n",
        "current = \"\"\n",
        "for ch in text:\n",
        " if ch != \" \":\n",
        "  current += ch\n",
        "else:\n",
        " words.append(current)\n",
        " current = \"\"\n",
        "words.append(current)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Word Tokens:\", words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character Tokenization"
      ],
      "metadata": {
        "id": "V2T1aNyLHLZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP Lab\"\n",
        "characters = []\n",
        "for ch in text:\n",
        " characters.append(ch)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Character Tokens:\", characters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cod4FBVFHMVA",
        "outputId": "8d70c3cd-e421-40d9-e580-afaae59aab30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLP Lab\n",
            "Character Tokens: ['N', 'L', 'P', ' ', 'L', 'a', 'b']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword tokenization\n"
      ],
      "metadata": {
        "id": "cNAgPcYHHVK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP's future, machine learning; deep:networks\"\n",
        "delimiters = [\" \", \"'\", \",\", \";\", \":\"]\n",
        "subwords = []\n",
        "current = \"\"\n",
        "for ch in text:\n",
        " if ch not in delimiters:\n",
        "  current += ch\n",
        " else:\n",
        "  if current != \"\":\n",
        "   subwords.append(current)\n",
        " current = \"\"\n",
        "if current != \"\":\n",
        " subwords.append(current)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Subword Tokens:\", subwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVq2_J69HY3q",
        "outputId": "c123c444-9d70-4029-c0db-1245399f7961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLP's future, machine learning; deep:networks\n",
            "Subword Tokens: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop word Removal"
      ],
      "metadata": {
        "id": "TJOAve42Hpdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "text = \"this is a simple example of stop word removal\"\n",
        "stop_words = [\"is\", \"a\", \"of\", \"this\", \"the\", \"and\", \"to\"]\n",
        "words = []\n",
        "current = \"\"\n",
        "for ch in text:\n",
        " if ch != \" \":\n",
        "  current += ch\n",
        " else:\n",
        "  words.append(current)\n",
        "  current = \"\"\n",
        "words.append(current)\n",
        "filtered_words = []\n",
        "for word in words:\n",
        " if word not in stop_words:\n",
        "  filtered_words.append(word)\n",
        "print(\"Original Words:\", words)\n",
        "print(\"After Stop Word Removal:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brM4nahrHscj",
        "outputId": "b1aadcca-3eea-467d-8122-420261dfd16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['this', 'is', 'a', 'simple', 'example', 'of', 'stop', 'word', 'removal']\n",
            "After Stop Word Removal: ['simple', 'example', 'stop', 'word', 'removal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "text = \"Write anything whatever you want here\"\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(text)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-JCCn8PYZ4O",
        "outputId": "1b56cc66-e065-4149-d872-f8f0d8428163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['write', 'anyth', 'whatev', 'you', 'want', 'here']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "text = \"Ram is going to temple daily\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fu_VDC5eFTY",
        "outputId": "558b6431-1d06-4673-cf59-7f4567daeefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ram', 'NNP'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('temple', 'VB'), ('daily', 'JJ')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ram is going to temple daily\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJcjoayxelP-",
        "outputId": "7af590a4-f7f4-4750-b7fd-ee6ff238fdae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ram', 'NNP'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('temple', 'VB'), ('daily', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgwR5x4DjsDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}